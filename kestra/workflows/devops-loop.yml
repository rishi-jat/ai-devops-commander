id: devops-loop
namespace: ai.devops.commander

description: |
  AI DevOps Commander - Real-time deployment monitoring with AI decisions
  
  This workflow:
  1. Receives deployment webhook
  2. Analyzes deployment health
  3. Makes ROLLBACK or CONTINUE decision
  4. Outputs results to dashboard

inputs:
  - id: deploymentId
    type: STRING
    defaults: deploy-001
  
  - id: service
    type: STRING
    defaults: payment-service
  
  - id: environment
    type: STRING
    defaults: production
    
  - id: version
    type: STRING
    defaults: v1.0.0
    
  - id: description
    type: STRING
    defaults: "Deployment triggered"


tasks:
  # Analyze the deployment
  - id: analyze_deployment
    type: io.kestra.plugin.core.log.Log
    message: "Analyzing deployment {{ inputs.deploymentId }} for {{ inputs.service }}"
    level: INFO
    
  # Simulate AI decision based on deployment ID
  - id: make_decision
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - |
        # Simulate AI analysis based on deployment patterns
        DEPLOYMENT_ID="{{ inputs.deploymentId }}"
        
        # Pattern matching for demo scenarios
        if [[ "$DEPLOYMENT_ID" == *"001"* ]] || [[ "$DEPLOYMENT_ID" == *"003"* ]]; then
          # Problematic deployments
          DECISION="ROLLBACK"
          SUMMARY="Critical issues detected. Memory leak or database timeout affecting production."
          REASONING="Error rate exceeded 30%. Service health degraded. Immediate rollback required."
          CONFIDENCE="0.92"
          HEALTH_SCORE="28"
          ERROR_RATE="35.4%"
          MEMORY="92%"
        else
          # Healthy deployments  
          DECISION="CONTINUE"
          SUMMARY="Deployment successful. All metrics within normal ranges."
          REASONING="All health indicators green. No anomalies detected. Safe to continue."
          CONFIDENCE="0.95"
          HEALTH_SCORE="94"
          ERROR_RATE="0.8%"
          MEMORY="48%"
        fi
        
        # Output structured data for dashboard
        echo "AI_DECISION=$DECISION" >> {{ outputDir }}/decision.txt
        echo "AI_SUMMARY=$SUMMARY" >> {{ outputDir }}/decision.txt
        echo "AI_REASONING=$REASONING" >> {{ outputDir }}/decision.txt
        echo "AI_CONFIDENCE=$CONFIDENCE" >> {{ outputDir }}/decision.txt
        echo "HEALTH_SCORE=$HEALTH_SCORE" >> {{ outputDir }}/decision.txt
        echo "ERROR_RATE=$ERROR_RATE" >> {{ outputDir }}/decision.txt
        echo "MEMORY_USAGE=$MEMORY" >> {{ outputDir }}/decision.txt
        
        echo "Decision: $DECISION with confidence $CONFIDENCE"
    
  # Log the decision
  - id: log_decision
    type: io.kestra.plugin.core.log.Log
    message: "AI Decision: {{ outputs.make_decision.vars.AI_DECISION }}"
    level: INFO

# Output the results so dashboard can consume them
outputs:
  - id: ai_decision
    type: STRING
    value: "{{ read(outputs.make_decision.outputFiles['decision.txt']) | regex_extract('AI_DECISION=(.*)') }}"
    
  - id: ai_summary
    type: STRING
    value: "{{ read(outputs.make_decision.outputFiles['decision.txt']) | regex_extract('AI_SUMMARY=(.*)') }}"
    
  - id: ai_reasoning
    type: STRING
    value: "{{ read(outputs.make_decision.outputFiles['decision.txt']) | regex_extract('AI_REASONING=(.*)') }}"
    
  - id: ai_confidence
    type: STRING  
    value: "{{ read(outputs.make_decision.outputFiles['decision.txt']) | regex_extract('AI_CONFIDENCE=(.*)') }}"
    
  - id: health_score
    type: STRING
    value: "{{ read(outputs.make_decision.outputFiles['decision.txt']) | regex_extract('HEALTH_SCORE=(.*)') }}"
    
  - id: error_rate
    type: STRING
    value: "{{ read(outputs.make_decision.outputFiles['decision.txt']) | regex_extract('ERROR_RATE=(.*)') }}"
    
  - id: memory_usage
    type: STRING
    value: "{{ read(outputs.make_decision.outputFiles['decision.txt']) | regex_extract('MEMORY_USAGE=(.*)') }}"
      client = OpenAI(
          api_key=os.getenv("TOGETHER_API_KEY", "demo-key"),
          base_url="https://api.together.xyz/v1"
      )
      
      # For demo, we'll use structured decision logic
      # In production deployment, this would be actual AI call
      
      health_score = data['health_score']
      error_rate = data['metrics']['error_rate_percent']
      memory_usage = data['metrics']['memory_usage_percent']
      
      # Decision logic
      if health_score < 50 or error_rate > 30 or memory_usage > 90:
          status = "CRITICAL"
          recommendation = "ROLLBACK"
          confidence = 95
          issue = "Critical system degradation detected"
      elif health_score < 70 or error_rate > 10:
          status = "DEGRADED"
          recommendation = "ROLLBACK"
          confidence = 85
          issue = "Service performance below acceptable thresholds"
      else:
          status = "HEALTHY"
          recommendation = "CONTINUE"
          confidence = 97
          issue = "All systems operational"
      
      summary = {
          'status': status,
          'issue': issue,
          'impact': f"Affecting {data['service']} at {error_rate:.1f}% error rate",
          'recommendation': recommendation,
          'confidence': confidence,
          'reasoning': f"Health score: {health_score}/100, Error rate: {error_rate}%, Memory: {memory_usage}%",
          'timestamp': '{{ execution.startDate }}'
      }
      
      print(json.dumps(summary, indent=2))
      
  # STEP 5: MAKE DECISION (Based on AI summary + RL model)
  - id: make_decision
    type: io.kestra.plugin.core.flow.Switch
    value: "{{ outputs.ai_summarize.vars.recommendation }}"
    cases:
      ROLLBACK:
        - id: execute_rollback
          type: io.kestra.plugin.core.log.Log
          message: |
            ðŸš¨ DECISION: ROLLBACK
            Deployment: {{ inputs.deployment_id }}
            Reason: {{ outputs.ai_summarize.vars.issue }}
            Confidence: {{ outputs.ai_summarize.vars.confidence }}%
            
            Executing automatic rollback...
        
        - id: rollback_action
          type: io.kestra.plugin.scripts.shell.Commands
          description: Execute rollback to previous version
          commands:
            - echo "Rolling back {{ inputs.service_name }} from {{ inputs.version }}"
            - echo "Previous stable version restored"
            - echo "Rollback completed successfully"
      
      CONTINUE:
        - id: continue_monitoring
          type: io.kestra.plugin.core.log.Log
          message: |
            âœ… DECISION: CONTINUE
            Deployment: {{ inputs.deployment_id }}
            Status: {{ outputs.ai_summarize.vars.status }}
            Confidence: {{ outputs.ai_summarize.vars.confidence }}%
            
            Deployment healthy. Continuing monitoring...
            
  # STEP 6: RECORD OUTCOME (For RL learning)
  - id: record_outcome
    type: io.kestra.plugin.scripts.python.Script
    description: Record deployment outcome for reinforcement learning model
    docker:
      image: python:3.11-slim
    script: |

triggers:
  - id: deployment_webhook
    type: io.kestra.plugin.core.trigger.Webhook
    key: "deployment-webhook"

labels:
  project: ai-devops-commander
  hackathon: ai-agents-assemble
  sponsor: kestra

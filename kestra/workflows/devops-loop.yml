id: devops-loop
namespace: ai.devops.commander

description: |
  AI DevOps Commander - Real-time deployment monitoring with AI decisions
  
  This workflow:
  1. Receives deployment webhook
  2. Analyzes deployment health
  3. Makes ROLLBACK or CONTINUE decision
  4. Outputs results to dashboard

inputs:
  - id: deploymentId
    type: STRING
    defaults: deploy-001
  
  - id: service
    type: STRING
    defaults: payment-service
  
  - id: environment
    type: STRING
    defaults: production
    
  - id: version
    type: STRING
    defaults: v1.0.0
    
  - id: description
    type: STRING
    defaults: "Deployment triggered"


tasks:
  # Generate random metrics for realistic demo
  - id: generate_metrics
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - |
        # Generate realistic deployment metrics
        ERROR_RATE=$(awk -v min=0.5 -v max=45.0 'BEGIN{srand(); print min+rand()*(max-min)}')
        MEMORY_USAGE=$(awk -v min=40 -v max=95 'BEGIN{srand(); print int(min+rand()*(max-min))}')
        CPU_USAGE=$(awk -v min=30 -v max=85 'BEGIN{srand(); print int(min+rand()*(max-min))}')
        RESPONSE_TIME=$(awk -v min=80 -v max=3000 'BEGIN{srand(); print int(min+rand()*(max-min))}')
        
        # Simulate log errors
        if (( $(echo "$ERROR_RATE > 10" | bc -l) )); then
          LOGS="ERROR: OutOfMemoryError in payment processor
        ERROR: Database connection timeout after 30s
        WARN: High memory pressure detected
        ERROR: Failed to process 234 transactions"
        else
          LOGS="INFO: Deployment successful
        INFO: All health checks passed
        INFO: Transaction processing normal
        INFO: Memory usage within limits"
        fi
        
        echo "ERROR_RATE=$ERROR_RATE" >> {{ outputDir }}/metrics.txt
        echo "MEMORY_USAGE=$MEMORY_USAGE" >> {{ outputDir }}/metrics.txt
        echo "CPU_USAGE=$CPU_USAGE" >> {{ outputDir }}/metrics.txt
        echo "RESPONSE_TIME=$RESPONSE_TIME" >> {{ outputDir }}/metrics.txt
        echo "LOGS<<EOF" >> {{ outputDir }}/metrics.txt
        echo "$LOGS" >> {{ outputDir }}/metrics.txt
        echo "EOF" >> {{ outputDir }}/metrics.txt
        
        echo "Generated metrics: Error=$ERROR_RATE%, Memory=$MEMORY_USAGE%, CPU=$CPU_USAGE%, Response=${RESPONSE_TIME}ms"
    
  # REAL AI DECISION using Together AI (Prize Category: Together AI)
  - id: ai_decision
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install requests openai
    env:
      DEPLOYMENT_ID: "{{ inputs.deploymentId }}"
      SERVICE: "{{ inputs.service }}"
      ENVIRONMENT: "{{ inputs.environment }}"
      VERSION: "{{ inputs.version }}"
      TOGETHER_API_KEY: "{{ secret('TOGETHER_API_KEY') }}"  # Set in Kestra secrets or use fallback
    script: |
      import os
      import json
      import random
      
      # Read generated metrics
      metrics = {}
      with open('{{ outputs.generate_metrics.outputFiles["metrics.txt"] }}', 'r') as f:
          content = f.read()
          for line in content.split('\n'):
              if '=' in line and '<<' not in line:
                  key, val = line.split('=', 1)
                  metrics[key] = val
      
      error_rate = float(metrics.get('ERROR_RATE', '0'))
      memory = int(metrics.get('MEMORY_USAGE', '50'))
      cpu = int(metrics.get('CPU_USAGE', '50'))
      response_time = int(metrics.get('RESPONSE_TIME', '100'))
      
      # Try to use real Together AI API
      use_real_ai = os.getenv('TOGETHER_API_KEY') and os.getenv('TOGETHER_API_KEY') != 'demo-key'
      
      if use_real_ai:
          try:
              from openai import OpenAI
              
              client = OpenAI(
                  api_key=os.getenv('TOGETHER_API_KEY'),
                  base_url="https://api.together.xyz/v1"
              )
              
              prompt = f"""You are an expert DevOps AI analyzing a production deployment.

Deployment Details:
- Service: {os.getenv('SERVICE')}
- Version: {os.getenv('VERSION')}
- Environment: {os.getenv('ENVIRONMENT')}

Current Metrics:
- Error Rate: {error_rate:.1f}%
- Memory Usage: {memory}%
- CPU Usage: {cpu}%
- Response Time: {response_time}ms

Based on these metrics, should we ROLLBACK or CONTINUE this deployment?

Respond in JSON format:
{{
  "decision": "ROLLBACK" or "CONTINUE",
  "confidence": 0.0-1.0,
  "summary": "brief summary",
  "reasoning": "detailed reasoning"
}}"""

              response = client.chat.completions.create(
                  model="meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo",
                  messages=[{"role": "user", "content": prompt}],
                  temperature=0.7,
                  max_tokens=500
              )
              
              ai_response = response.choices[0].message.content
              
              # Parse AI response
              try:
                  ai_data = json.loads(ai_response)
                  decision = ai_data.get('decision', 'CONTINUE')
                  confidence = float(ai_data.get('confidence', 0.85))
                  summary = ai_data.get('summary', 'AI analysis completed')
                  reasoning = ai_data.get('reasoning', 'Based on metric analysis')
              except:
                  # Fallback if AI returns non-JSON
                  decision = "ROLLBACK" if error_rate > 15 else "CONTINUE"
                  confidence = 0.85
                  summary = f"AI analysis: {ai_response[:100]}"
                  reasoning = ai_response[:200]
              
              print(f"‚úÖ Used REAL Together AI: {response.model}")
              
          except Exception as e:
              print(f"‚ö†Ô∏è  Together AI failed ({str(e)}), using fallback logic")
              use_real_ai = False
      
      # Fallback: Smart algorithmic decision (if no API key or API fails)
      if not use_real_ai:
          # Calculate health score
          health_score = 100
          health_score -= min(error_rate * 2, 50)
          health_score -= max((memory - 70) * 0.5, 0)
          health_score -= max((response_time - 500) * 0.01, 0)
          health_score = max(0, int(health_score))
          
          # AI Decision criteria
          critical_issues = []
          if error_rate > 15:
              critical_issues.append(f"Error rate {error_rate:.1f}% exceeds 15% threshold")
          if memory > 85:
              critical_issues.append(f"Memory usage {memory}% indicates possible leak")
          if response_time > 2000:
              critical_issues.append(f"Response time {response_time}ms exceeds SLA")
          
          # Make decision
          if health_score < 50 or len(critical_issues) >= 2:
              decision = "ROLLBACK"
              confidence = 0.88 + random.uniform(0, 0.10)
              summary = f"Critical issues detected. Health score: {health_score}/100. Immediate action required."
              reasoning = f"AI detected {len(critical_issues)} critical issues: {'; '.join(critical_issues)}. Automatic rollback recommended to prevent service degradation."
          else:
              decision = "CONTINUE"
              confidence = 0.90 + random.uniform(0, 0.08)
              summary = f"Deployment healthy. Health score: {health_score}/100. All metrics within acceptable ranges."
              reasoning = f"Error rate {error_rate:.1f}% is acceptable, memory at {memory}%, response time {response_time}ms. No anomalies detected. Safe to proceed."
          
          health_score_value = health_score
      else:
          # Calculate health score for real AI too
          health_score_value = max(0, 100 - int(error_rate * 2) - max((memory - 70) * 0.5, 0))
      
      # Output results
      result = {
          "ai_decision": decision,
          "ai_confidence": round(confidence, 2) if isinstance(confidence, float) else confidence,
          "ai_summary": summary,
          "ai_reasoning": reasoning,
          "health_score": health_score_value,
          "error_rate": f"{error_rate:.1f}%",
          "memory_usage": f"{memory}%",
          "response_time": f"{response_time}ms",
          "ai_provider": "together-ai" if use_real_ai else "algorithmic"
      }
      
      print(json.dumps(result, indent=2))
      
      # Save for outputs
      with open('{{ outputDir }}/decision.json', 'w') as f:
          json.dump(result, f)
  
  # Collect training data for Oumi RL (Prize Category: Oumi RL)
  - id: collect_training_data
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    script: |
      import json
      from datetime import datetime
      
      # Load decision data
      with open('{{ outputs.ai_decision.outputFiles["decision.json"] }}', 'r') as f:
          decision_data = json.load(f)
      
      # Create RL training sample
      training_sample = {
          "timestamp": "{{ execution.startDate }}",
          "execution_id": "{{ execution.id }}",
          "state": {
              "deployment_id": "{{ inputs.deploymentId }}",
              "service": "{{ inputs.service }}",
              "environment": "{{ inputs.environment }}",
              "error_rate": decision_data.get("error_rate"),
              "memory_usage": decision_data.get("memory_usage"),
              "response_time": decision_data.get("response_time"),
              "health_score": decision_data.get("health_score")
          },
          "action": decision_data.get("ai_decision"),
          "confidence": decision_data.get("ai_confidence"),
          "reward": None,  # Will be updated after observing outcome
          "reasoning": decision_data.get("ai_reasoning")
      }
      
      print("üìä RL Training Sample Collected:")
      print(json.dumps(training_sample, indent=2))
      
      # Save for RL model training
      with open('{{ outputDir }}/rl_training_sample.json', 'w') as f:
          json.dump(training_sample, f)
      
      print("\n‚úÖ Data ready for Oumi RL training pipeline")
  
  # Trigger Cline automation if ROLLBACK (Prize Category: Cline)
  - id: cline_automation
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - |
        DECISION="{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_decision }}"
        
        if [ "$DECISION" = "ROLLBACK" ]; then
          echo "ü§ñ Triggering Cline Automation for automated remediation..."
          
          # Extract error type from reasoning
          REASONING="{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_reasoning }}"
          
          # Determine error type
          ERROR_TYPE="generic"
          if echo "$REASONING" | grep -i "memory" > /dev/null; then
            ERROR_TYPE="memory-leak"
          elif echo "$REASONING" | grep -i "database\|timeout" > /dev/null; then
            ERROR_TYPE="database-timeout"
          elif echo "$REASONING" | grep -i "response\|latency" > /dev/null; then
            ERROR_TYPE="performance"
          fi
          
          echo "Error Type Detected: $ERROR_TYPE"
          echo "Deployment ID: {{ inputs.deploymentId }}"
          echo "Service: {{ inputs.service }}"
          
          # In production, this would call actual Cline automation scripts
          echo "‚úÖ Cline automation would:"
          echo "  1. Analyze error patterns"
          echo "  2. Apply automated fixes"
          echo "  3. Create PR with changes"
          echo "  4. Run tests on fix"
          echo "  5. Deploy if tests pass"
          
          echo ""
          echo "üìù Automated Fix Plan:"
          case "$ERROR_TYPE" in
            memory-leak)
              echo "  - Increase heap size"
              echo "  - Add memory monitoring"
              echo "  - Fix resource leaks in code"
              ;;
            database-timeout)
              echo "  - Increase connection pool"
              echo "  - Optimize slow queries"
              echo "  - Add query timeout handlers"
              ;;
            performance)
              echo "  - Enable caching layer"
              echo "  - Add CDN configuration"
              echo "  - Optimize database queries"
              ;;
            *)
              echo "  - Rollback to stable version"
              echo "  - Collect diagnostic logs"
              echo "  - Alert on-call team"
              ;;
          esac
          
          echo ""
          echo "‚úÖ Cline automation triggered successfully!"
        else
          echo "‚úÖ Deployment healthy - no automation needed"
        fi
    
  # Log the decision
  - id: log_decision
    type: io.kestra.plugin.core.log.Log
    message: "AI Decision: {{ read(outputs.ai_decision.outputFiles['decision.json']) }}"
    level: INFO

# Output the results so dashboard can consume them
outputs:
  - id: ai_decision
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_decision }}"
    
  - id: ai_summary
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_summary }}"
    
  - id: ai_reasoning
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_reasoning }}"
    
  - id: ai_confidence
    type: STRING  
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_confidence }}"
    
  - id: health_score
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).health_score }}"
    
  - id: error_rate
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).error_rate }}"
    
  - id: memory_usage
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).memory_usage }}"
          confidence = 97
          issue = "All systems operational"
      
      summary = {
          'status': status,
          'issue': issue,
          'impact': f"Affecting {data['service']} at {error_rate:.1f}% error rate",
          'recommendation': recommendation,
          'confidence': confidence,
          'reasoning': f"Health score: {health_score}/100, Error rate: {error_rate}%, Memory: {memory_usage}%",
          'timestamp': '{{ execution.startDate }}'
      }
      
      print(json.dumps(summary, indent=2))
      
  # STEP 5: MAKE DECISION (Based on AI summary + RL model)
  - id: make_decision
    type: io.kestra.plugin.core.flow.Switch
    value: "{{ outputs.ai_summarize.vars.recommendation }}"
    cases:
      ROLLBACK:
        - id: execute_rollback
          type: io.kestra.plugin.core.log.Log
          message: |
            üö® DECISION: ROLLBACK
            Deployment: {{ inputs.deployment_id }}
            Reason: {{ outputs.ai_summarize.vars.issue }}
            Confidence: {{ outputs.ai_summarize.vars.confidence }}%
            
            Executing automatic rollback...
        
        - id: rollback_action
          type: io.kestra.plugin.scripts.shell.Commands
          description: Execute rollback to previous version
          commands:
            - echo "Rolling back {{ inputs.service_name }} from {{ inputs.version }}"
            - echo "Previous stable version restored"
            - echo "Rollback completed successfully"
      
      CONTINUE:
        - id: continue_monitoring
          type: io.kestra.plugin.core.log.Log
          message: |
            ‚úÖ DECISION: CONTINUE
            Deployment: {{ inputs.deployment_id }}
            Status: {{ outputs.ai_summarize.vars.status }}
            Confidence: {{ outputs.ai_summarize.vars.confidence }}%
            
            Deployment healthy. Continuing monitoring...
            
  # STEP 6: RECORD OUTCOME (For RL learning)
  - id: record_outcome
    type: io.kestra.plugin.scripts.python.Script
    description: Record deployment outcome for reinforcement learning model
    docker:
      image: python:3.11-slim
    script: |

triggers:
  - id: deployment_webhook
    type: io.kestra.plugin.core.trigger.Webhook
    key: "deployment-webhook"

labels:
  project: ai-devops-commander
  hackathon: ai-agents-assemble
  sponsor: kestra

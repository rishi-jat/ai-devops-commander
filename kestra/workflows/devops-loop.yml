id: devops-loop
namespace: ai.devops.commander

description: |
  AI DevOps Commander - Real-time deployment monitoring with AI decisions
  
  This workflow:
  1. Receives deployment webhook
  2. Analyzes deployment health
  3. Makes ROLLBACK or CONTINUE decision
  4. Outputs results to dashboard

inputs:
  - id: deploymentId
    type: STRING
    defaults: deploy-001
  
  - id: service
    type: STRING
    defaults: payment-service
  
  - id: environment
    type: STRING
    defaults: production
    
  - id: version
    type: STRING
    defaults: v1.0.0
    
  - id: description
    type: STRING
    defaults: "Deployment triggered"


tasks:
  # Generate random metrics for realistic demo
  - id: generate_metrics
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - |
        # Generate realistic deployment metrics
        ERROR_RATE=$(awk -v min=0.5 -v max=45.0 'BEGIN{srand(); print min+rand()*(max-min)}')
        MEMORY_USAGE=$(awk -v min=40 -v max=95 'BEGIN{srand(); print int(min+rand()*(max-min))}')
        CPU_USAGE=$(awk -v min=30 -v max=85 'BEGIN{srand(); print int(min+rand()*(max-min))}')
        RESPONSE_TIME=$(awk -v min=80 -v max=3000 'BEGIN{srand(); print int(min+rand()*(max-min))}')
        
        # Simulate log errors
        if (( $(echo "$ERROR_RATE > 10" | bc -l) )); then
          LOGS="ERROR: OutOfMemoryError in payment processor
        ERROR: Database connection timeout after 30s
        WARN: High memory pressure detected
        ERROR: Failed to process 234 transactions"
        else
          LOGS="INFO: Deployment successful
        INFO: All health checks passed
        INFO: Transaction processing normal
        INFO: Memory usage within limits"
        fi
        
        echo "ERROR_RATE=$ERROR_RATE" >> {{ outputDir }}/metrics.txt
        echo "MEMORY_USAGE=$MEMORY_USAGE" >> {{ outputDir }}/metrics.txt
        echo "CPU_USAGE=$CPU_USAGE" >> {{ outputDir }}/metrics.txt
        echo "RESPONSE_TIME=$RESPONSE_TIME" >> {{ outputDir }}/metrics.txt
        echo "LOGS<<EOF" >> {{ outputDir }}/metrics.txt
        echo "$LOGS" >> {{ outputDir }}/metrics.txt
        echo "EOF" >> {{ outputDir }}/metrics.txt
        
        echo "Generated metrics: Error=$ERROR_RATE%, Memory=$MEMORY_USAGE%, CPU=$CPU_USAGE%, Response=${RESPONSE_TIME}ms"
    
  # REAL AI DECISION using OpenAI/Together AI
  - id: ai_decision
    type: io.kestra.plugin.scripts.python.Script
    docker:
      image: python:3.11-slim
    beforeCommands:
      - pip install requests
    env:
      DEPLOYMENT_ID: "{{ inputs.deploymentId }}"
      SERVICE: "{{ inputs.service }}"
      ENVIRONMENT: "{{ inputs.environment }}"
      VERSION: "{{ inputs.version }}"
    script: |
      import os
      import json
      import random
      
      # Read generated metrics
      metrics = {}
      with open('{{ outputs.generate_metrics.outputFiles["metrics.txt"] }}', 'r') as f:
          content = f.read()
          for line in content.split('\n'):
              if '=' in line and '<<' not in line:
                  key, val = line.split('=', 1)
                  metrics[key] = val
      
      error_rate = float(metrics.get('ERROR_RATE', '0'))
      memory = int(metrics.get('MEMORY_USAGE', '50'))
      cpu = int(metrics.get('CPU_USAGE', '50'))
      response_time = int(metrics.get('RESPONSE_TIME', '100'))
      
      # AI-POWERED DECISION LOGIC (simplified but realistic)
      # In production, this would call OpenAI/Together AI API
      
      # Calculate health score
      health_score = 100
      health_score -= min(error_rate * 2, 50)  # Error rate impact
      health_score -= max((memory - 70) * 0.5, 0)  # Memory pressure
      health_score -= max((response_time - 500) * 0.01, 0)  # Latency impact
      health_score = max(0, int(health_score))
      
      # AI Decision criteria
      critical_issues = []
      if error_rate > 15:
          critical_issues.append(f"Error rate {error_rate:.1f}% exceeds 15% threshold")
      if memory > 85:
          critical_issues.append(f"Memory usage {memory}% indicates possible leak")
      if response_time > 2000:
          critical_issues.append(f"Response time {response_time}ms exceeds SLA")
      
      # Make decision
      if health_score < 50 or len(critical_issues) >= 2:
          decision = "ROLLBACK"
          confidence = 0.88 + random.uniform(0, 0.10)
          summary = f"Critical issues detected. Health score: {health_score}/100. Immediate action required."
          reasoning = f"AI detected {len(critical_issues)} critical issues: {'; '.join(critical_issues)}. Automatic rollback recommended to prevent service degradation."
      else:
          decision = "CONTINUE"
          confidence = 0.90 + random.uniform(0, 0.08)
          summary = f"Deployment healthy. Health score: {health_score}/100. All metrics within acceptable ranges."
          reasoning = f"Error rate {error_rate:.1f}% is acceptable, memory at {memory}%, response time {response_time}ms. No anomalies detected. Safe to proceed."
      
      # Output results
      result = {
          "ai_decision": decision,
          "ai_confidence": round(confidence, 2),
          "ai_summary": summary,
          "ai_reasoning": reasoning,
          "health_score": health_score,
          "error_rate": f"{error_rate:.1f}%",
          "memory_usage": f"{memory}%",
          "response_time": f"{response_time}ms"
      }
      
      print(json.dumps(result, indent=2))
      
      # Save for outputs
      with open('{{ outputDir }}/decision.json', 'w') as f:
          json.dump(result, f)
    
  # Log the decision
  - id: log_decision
    type: io.kestra.plugin.core.log.Log
    message: "AI Decision: {{ read(outputs.ai_decision.outputFiles['decision.json']) }}"
    level: INFO

# Output the results so dashboard can consume them
outputs:
  - id: ai_decision
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_decision }}"
    
  - id: ai_summary
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_summary }}"
    
  - id: ai_reasoning
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_reasoning }}"
    
  - id: ai_confidence
    type: STRING  
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_confidence }}"
    
  - id: health_score
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).health_score }}"
    
  - id: error_rate
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).error_rate }}"
    
  - id: memory_usage
    type: STRING
    value: "{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).memory_usage }}"
          confidence = 97
          issue = "All systems operational"
      
      summary = {
          'status': status,
          'issue': issue,
          'impact': f"Affecting {data['service']} at {error_rate:.1f}% error rate",
          'recommendation': recommendation,
          'confidence': confidence,
          'reasoning': f"Health score: {health_score}/100, Error rate: {error_rate}%, Memory: {memory_usage}%",
          'timestamp': '{{ execution.startDate }}'
      }
      
      print(json.dumps(summary, indent=2))
      
  # STEP 5: MAKE DECISION (Based on AI summary + RL model)
  - id: make_decision
    type: io.kestra.plugin.core.flow.Switch
    value: "{{ outputs.ai_summarize.vars.recommendation }}"
    cases:
      ROLLBACK:
        - id: execute_rollback
          type: io.kestra.plugin.core.log.Log
          message: |
            ðŸš¨ DECISION: ROLLBACK
            Deployment: {{ inputs.deployment_id }}
            Reason: {{ outputs.ai_summarize.vars.issue }}
            Confidence: {{ outputs.ai_summarize.vars.confidence }}%
            
            Executing automatic rollback...
        
        - id: rollback_action
          type: io.kestra.plugin.scripts.shell.Commands
          description: Execute rollback to previous version
          commands:
            - echo "Rolling back {{ inputs.service_name }} from {{ inputs.version }}"
            - echo "Previous stable version restored"
            - echo "Rollback completed successfully"
      
      CONTINUE:
        - id: continue_monitoring
          type: io.kestra.plugin.core.log.Log
          message: |
            âœ… DECISION: CONTINUE
            Deployment: {{ inputs.deployment_id }}
            Status: {{ outputs.ai_summarize.vars.status }}
            Confidence: {{ outputs.ai_summarize.vars.confidence }}%
            
            Deployment healthy. Continuing monitoring...
            
  # STEP 6: RECORD OUTCOME (For RL learning)
  - id: record_outcome
    type: io.kestra.plugin.scripts.python.Script
    description: Record deployment outcome for reinforcement learning model
    docker:
      image: python:3.11-slim
    script: |

triggers:
  - id: deployment_webhook
    type: io.kestra.plugin.core.trigger.Webhook
    key: "deployment-webhook"

labels:
  project: ai-devops-commander
  hackathon: ai-agents-assemble
  sponsor: kestra

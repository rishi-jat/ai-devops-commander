id: devops-loop
namespace: ai.devops.commander

description: |
  AI DevOps Commander - Real-time deployment monitoring with AI decisions
  
  This workflow:
  1. Receives deployment webhook
  2. Analyzes deployment health
  3. Makes ROLLBACK or CONTINUE decision
  4. Outputs results to dashboard

inputs:
  - id: deploymentId
    type: STRING
    defaults: deploy-001
  
  - id: service
    type: STRING
    defaults: payment-service
  
  - id: environment
    type: STRING
    defaults: production
    
  - id: version
    type: STRING
    defaults: v1.0.0
    
  - id: description
    type: STRING
    defaults: "Deployment triggered"

tasks:
  - id: generate_metrics
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - |
        # Generate realistic deployment metrics
        ERROR_RATE=$(awk -v min=0.5 -v max=45.0 'BEGIN{srand(); print min+rand()*(max-min)}')
        MEMORY_USAGE=$(awk -v min=40 -v max=95 'BEGIN{srand(); print int(min+rand()*(max-min))}')
        CPU_USAGE=$(awk -v min=30 -v max=85 'BEGIN{srand(); print int(min+rand()*(max-min))}')
        RESPONSE_TIME=$(awk -v min=80 -v max=3000 'BEGIN{srand(); print int(min+rand()*(max-min))}')
        
        echo "{\"error_rate\": $ERROR_RATE, \"memory_usage\": $MEMORY_USAGE, \"cpu_usage\": $CPU_USAGE, \"response_time\": $RESPONSE_TIME}" > {{ outputDir }}/metrics.json
        
        echo "Generated metrics: Error=$ERROR_RATE percent, Memory=$MEMORY_USAGE percent, CPU=$CPU_USAGE percent, Response=${RESPONSE_TIME}ms"
    outputs:
      - id: metrics
        type: JSON
        value: "{{ read(outputDir + '/metrics.json') }}"
    
  - id: ai_decision
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:3.11-slim
    beforeCommands:
      - pip install requests
    script: |
      import os
      import json
      import random
      
      # Read generated metrics from previous task
      metrics_str = '''{{ outputs.generate_metrics.metrics }}'''
      metrics = json.loads(metrics_str)
      
      error_rate = float(metrics.get('error_rate', 0))
      memory = int(metrics.get('memory_usage', 50))
      cpu = int(metrics.get('cpu_usage', 50))
      response_time = int(metrics.get('response_time', 100))
      
      # Try to use real Together AI API
      use_real_ai = os.getenv('TOGETHER_API_KEY') and os.getenv('TOGETHER_API_KEY') != 'demo-key'
      
      if use_real_ai:
          try:
              from openai import OpenAI
              
              client = OpenAI(
                  api_key=os.getenv('TOGETHER_API_KEY'),
                  base_url="https://api.together.xyz/v1"
              )
              
        Smart algorithmic decision logic
      use_real_ai = False  # Set to True when you add TOGETHER_API_KEY secret in Kestra
      ds 15% threshold")
          if memory > 85:
              critical_issues.append(f"Memory usage {memory}% indicates possible leak")
          if response_time > 2000:
              critical_issues.append(f"Response time {response_time}ms exceeds SLA")
          
          # Make decision
          if health_score < 50 or len(critical_issues) >= 2:
              decision = "ROLLBACK"
              confidence = 0.88 + random.uniform(0, 0.10)
              summary = f"Critical issues detected. Health score: {health_score}/100. Immediate action required."
              reasoning = f"AI detected {len(critical_issues)} critical issues: {'; '.join(critical_issues)}. Automatic rollback recommended to prevent service degradation."
          else:
              decision = "CONTINUE"
              confidence = 0.90 + random.uniform(0, 0.08)
              summary = f"Deployment healthy. Health score: {health_score}/100. All metrics within acceptable ranges."
              reasoning = f"Error rate {error_rate:.1f}% is acceptable, memory at {memory}%, response time {response_time}ms. No anomalies detected. Safe to proceed."
          
          health_score_value = health_score
      else:
          # Calculate health score for real AI too
          health_score_value = max(0, 100 - int(error_rate * 2) - max((memory - 70) * 0.5, 0))
      
      # Output results
      result = {
          "ai_decision": decision,
          "ai_confidence": round(confidence, 2) if isinstance(confidence, float) else confidence,
          "ai_summary": summary,
          "ai_reasoning": reasoning,
          "health_score": health_score_value,
          "error_rate": f"{error_rate:.1f} percent",
          "memory_usage": f"{memory} percent",
          "response_time": f"{response_time}ms",
          "ai_provider": "algorithmic"
      }
      
      print(json.dumps(result, indent=2))
      
      with open('{{ outputDir }}/decision.json', 'w') as f:
          json.dump(result, f)
    outputs:
      - id: decision_data
  - id: collect_training_data
    type: io.kestra.plugin.scripts.python.Script
    taskRunner:
      type: io.kestra.plugin.scripts.runner.docker.Docker
    containerImage: python:3.11-slim
    script: |
      import json
      
      decision_str = '''{{ outputs.ai_decision.decision_data }}'''
      decision_data = json.loads(decision_str)
      
      training_sample = {
          "timestamp": "{{ execution.startDate }}",
          "execution_id": "{{ execution.id }}",
          "state": {
              "deployment_id": "{{ inputs.deploymentId }}",
              "service": "{{ inputs.service }}",
              "environment": "{{ inputs.environment }}",
              "error_rate": decision_data.get("error_rate"),
              "memory_usage": decision_data.get("memory_usage"),
              "response_time": decision_data.get("response_time"),
              "health_score": decision_data.get("health_score")
          },
          "action": decision_data.get("ai_decision"),
          "confidence": decision_data.get("ai_confidence"),
          "reward": None,
          "reasoning": decision_data.get("ai_reasoning")
      }
      
      print("RL Training Sample Collected:")
      print(json.dumps(training_sample, indent=2))
      
      with open('{{ outputDir }}/rl_training_sample.json', 'w') as f:
          json.dump(training_sample, f)
      
      print("soning = f"Error rate {error_rate:.1f} percent is acceptable, memory at {memory} percent, response time {response_time}ms. No anomalies detected. Safe to proceed."
          
          health_score_value = health_score
  - id: cline_automation
    type: io.kestra.plugin.scripts.shell.Commands
    commands:
      - |
        DECISION="{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_decision }}"
        
        if [ "$DECISION" = "ROLLBACK" ]; then
          echo "ü§ñ Triggering Cline Automation for automated remediation..."
          
          # Extract error type from reasoning
          REASONING="{{ json(read(outputs.ai_decision.outputFiles['decision.json'])).ai_reasoning }}"
          
          # Determine error type
          ERROR_TYPE="generic"
          if echo "$REASONING" | grep -i "memory" > /dev/null; then
            ERROR_TYPE="memory-leak"
          elif echo "$REASONING" | grep -i "database\|timeout" > /dev/null; then
            ERROR_TYPE="database-timeout"
          elif echo "$REASONING" | grep -i "response\|latency" > /dev/null; then
            ERROR_TYPE="performance"
          fi
          
          echo "Error Type Detected: $ERROR_TYPE"
          echo "Deployment ID: {{ inputs.deploymentId }}"
          echo "Service: {{ inputs.service }}"
          
          # In production, this would call actual Cline automation scripts
          echo "‚úÖ Cline automation would:"
          echo "  1. Analyze error patterns"
          echo "  2. Apply automated fixes"
          echo "  3. Create PR with changes"
          echo "  4. Run tests on fix"
          echo "  5. Deploy if tests pass"
          
          echo ""
          echo "üìù Automated Fix Plan:"
          case "$ERROR_TYPE" in
            memory-leak)
              echo "  - Increase heap size"
              echo "  - Add memory monitoring"
              echo "  - Fix resource leaks in code"
              ;;
            database-timeout)
              echo "  - Increase connection pool"
              echo "  - Optimize slow queries"
              echo "  - Add query timeout handlers"
              ;;
            performance)
              echo "  - Enable caching layer"
              echo "  - Add CDN configuration"
              echo "  - Optimize database queries"
              ;;
            *)
              echo "  - Rollback to stable version"
              echo "  - Collect diagnostic logs"
              echo "  - Alert on-call team"
              ;;
          esac
          
          echo ""
          echo "‚úÖ Cline automation triggered successfully!"
        else
          echo "‚úÖ Deployment healthy - no automation needed"
        fi
    
  # Log the decision
  - id: log_decision
    type: io.kestra.plugin.core.log.Log
    message: "AI Decision: {{ read(outputs.ai_decision.outputFiles['decision.json']) }}"

triggers:
  - id: deployment_webhook
    type: io.kestra.plugin.core.trigger.Webhook
    key: deployment-webhook
